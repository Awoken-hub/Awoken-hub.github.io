{"pages":[{"title":"欢迎","text":"👋 欢迎来到我的博客 这里是不眠觉的个人空间，感谢你的到访！ 在这里你可以看到我的技术文章、随笔 你可以通过顶部菜单进入分类和标签以查看你感兴趣的内容","link":"index.html"},{"title":"","text":"// 自动生成的时间戳文件 - 请勿手动编辑 window.LAST_DEPLOY_TIME = \"2025/09/16 20:50:40\"; // 在页面加载完成后更新时间戳显示 document.addEventListener('DOMContentLoaded', function() { const timestampElement = document.getElementById('last-deploy'); if (timestampElement) { timestampElement.textContent = window.LAST_DEPLOY_TIME; } // 同时查找其他可能的时间戳显示元素 const otherElements = document.querySelectorAll('.last-deploy-time, .deployment-time'); otherElements.forEach(element => { element.textContent = window.LAST_DEPLOY_TIME; }); });","link":"js/timestamp.js"}],"posts":[{"title":"Kinect for Windows 编程指南","text":"[原文](Kinect for Windows Programming Guide | Microsoft Learn) 欢迎使用 Kinect for Windows 软件开发工具包(SDK)2.0版本。该 SDK 提供了开发基于 Kinect 的 Microsoft Windows 应用程序所需的工具和 API （包括原生代码和托管代码）。开发基于 Kinect 的应用程序本质上与开发其他 Windows 应用程序相同，不同之处在于 Kinect SDK 提供了对 Kinect 特有功能的支持，包括彩色图像、深度图像、音频输入和骨骼数据。 以下是一些你可以使用本 SDK 支持的功能构建的 Windows 应用程序示例： 使用骨骼数据来识别和追踪移动中的人 使用深度数据来确定对象与传感器摄像头之间的距离 使用噪声和回声消除功能捕获音频，或者查找声源位置 通过为语音识别引擎编写语法规则，来实现可以语音激活的应用程序 Kinect for Windows SDK 包含的内容该 SDK 包括： 用于使用 Kinect for Windows 传感器开发 Kinect 应用的驱动程序和技术文档 用于托管和非托管代码编程的参考 API 和文档。这些 API 能在各种视频模式、CPU 负载和硬件环境下，以最小的软件延迟传递多路媒体数据流 展示使用 Kinect 传感器的良好实践的示例 将示例分解为用户任务的示例代码 本节内容 [[倾斜追踪]] 演示如何使用倾斜追踪 API [[人体追踪]] 讨论如何使用 Kinect 人体追踪功能 [[面部追踪]] 概述 Kinect 面部追踪的编程模型 [[高清面部追踪]] 讨论核心的高清面部追踪 API [[坐标映射]] 解释如何使用坐标映射在 Kinect 的各个坐标空间之中投影数据 [[Kinect API 概述]] 概述可用于 Kinect for Windows 应用程序的 API","link":"2025/09/12/Kinect for Windows 编程指南/"},{"title":"2022 摄影作品","text":"春干皱的花瓣落向新发的枝叶，雀跃天地间——春天是死亡与新生的合影。 摄于2022年春，大连。 秋 摄于2022年秋，北京。","link":"2025/09/16/2022 摄影作品/"},{"title":"Kinect API 概述","text":"原文 本文提供了可用于开发 Kinect for Windows 应用程序的 API 的高级概述。 Kinect for Windows SDK 2.0 提供了三套不同的 API 集来创建支持 Kinect 的应用程序：一套 Windows Runtime API (WinRT) 用于开发 Windows Store 应用，一套 .NET API 用于开发 WPT 应用程序，一套原生 API 用于开发需要原生代码性能优势的应用。 为清晰和易读起见，本主题中链接的 API 均来自 Windows Runtime API 。但所讨论的 API 和编程概念适用于所有三种 API 集。详细的 API 参考文档，请参阅参考。 视频数据 音频数据 视频数据来自 Kinect 传感器的视频数据包括低级数据（红外、彩色）以及处理后的数据（深度、人体骨骼）。这些 API 遵循标准的 WinRT 设计指南，支持所有与 WinRT 兼容的语言和 UI 框架。 传感器获取与启动Kinect for Windows 支持一个默认传感器。KinectSensor 类提供静态成员来帮助配置 Kinect 传感器并访问传感器数据。 关键 API 包括： KinectSensor.GetDefault 方法 KinectSensor.Open 方法 如果调用进程已调用 Open，则 KinectSensor.IsOpen 属性将返回 true。 如果没有可用的 Kinect 传感器，KinectSensor.IsAvailable 属性将返回 false。例如，如果用户安装了应用程序，但 PC 未连接 Kinect 传感器，则会返回 false。如果是这种情况，你的应用程序应该提示用户“传感器不可用”。 数据源传感器提供多个数据源，包括：彩色、深度、人体、人体索引和红外。在功能上，每个数据源由以下三个部分控制： 源类型 检查或配置数据源，并打开源读取器。 源读取器类型 使用事件或轮询获取数据源的帧。 帧类型 访问源中特定帧的数据。 关于以上内容在 API 中表示的示例，请参阅以下主题： FrameSourceTypes 枚举 FrameDescription 类 ColorImageFormat 枚举 ColorFrameSource.OpenReader 方法 数据流开发者打开传感器后，就可以使用实例访问各个数据流：彩色、深度、人体、人体索引、红外和长曝光红外。每个数据流都分为三部分： 流类型 用于检查或设置流，并打开流读取器。 流读取器类型 通过事件或轮询访问数据流的帧。 帧类型 访问流中特定帧的数据。 流类型的任务是让你访问流、允许配置（如果处于独占模式）和打开流读取器。每个流都有自己的类型，但它们都有相同的基本功能。 API 元素 描述 BodyIndexFrameSource.OpenReader 方法 打开此流的读取器。如果冲突的流已打开，此操作将失败。 BodyIndexFrameSource.IsActive 属性 指示此流是否有活动的流读取器。 BodyIndexFrameSource.FrameDescription 属性 提供关于输出帧的信息，以便你可以预分配正确大小的缓冲区来存储帧数据。不适用于 BodyFrameStream。 BodyIndexFrameSource.KinectSensor 属性 指向输出此流的传感器的反向指针。 数据流读取器每个数据流都有自己的流读取器类型，但它们都有相同的基本功能。 API 元素 描述 BodyFrameReader.AcquireLatestFrame 方法 获取最新帧。 BodyFrameReader.FrameArrived 事件 订阅此事件以便在有新帧可用时收到通知。 BodyFrameReader.IsPaused 属性 控制是否将帧传递给该读取器实例的用户。这对流本身或其他打开的流读取器没有影响。 BodyFrameReader.Close 方法 关闭流读取器。当最后一个打开的流读取器被关闭时，流本身将被禁用。 BodyFrameReader.BodyFrameSource 属性 指向打开此读取器的流的反向指针。 帧帧包含传感器采集的数据。使用帧对象将帧数据复制到应用程序的缓冲区或访问底层系统缓冲区。每个帧只会临时存储帧数据，应用程序应尽快从每个帧中获取数据并关闭/释放它，以释放底层句柄，并确保系统不需要持续分配新内存来存储传入的帧数据。 每个流都有自己的帧类型，它们都有相同的基本功能；尽管它们有很多共同点，但这些帧类型在不同的流之间的差异也非常大。 API 元素 描述 DepthFrame.CopyFrameDataToArray 方法 将帧的像素数据复制到应用程序提供的基于数组的缓冲区中。 DepthFrame.CopyFrameDataToBuffer 方法 将帧的像素数据复制到应用程序提供的 IBuffer 中。 DepthFrame.LockImageBuffer 方法 让应用程序访问系统用于存储此帧数据的底层缓冲区。长时间保持此缓冲区活动将导致系统持续为输入流分配新缓冲区。 无法尝试写入此缓冲区。 DepthFrame.Close 方法 释放底层系统缓冲区的句柄。调用此方法后，所有其他操作都将失败。 DepthFrame.DepthFrameSource 属性 指向提供此帧的流读取器的反向指针。 DepthFrame.FrameDescription 属性 提供应用程序需要了解的关于为此帧分配缓冲区的信息。不适用于 BodyFrame。 DepthFrame.RelativeTime 属性 提供一个 时间戳（UInt64），表示此帧的创建时间。此值不允许使用绝对的现实世界创建时间，但允许获取帧之间的相对交付时间（来自给定 Kinect 传感器的帧内和帧间），单位为毫秒。 红外帧InfraredFrame 类提供类似黑白照片的场景视图，但它是主动发光的，因此亮度不受位置和环境亮度的影响。红外帧非常适合依赖纹理信息的计算机视觉算法，例如面部识别。数据存储为 16 位无符号整数。红外帧也非常适合绿幕抠像、跟踪反光标记和滤除低返回的深度像素（这是造成抖动的原因）。请注意，红外帧与深度来自同一个传感器，因此图像完美对齐。例如，第 5 行第 9 列的红外像素与第 5 行第 9 列的深度像素相对应。 长曝光红外帧此帧类似于 InfraredFrame 类，但它的曝光时间更长，图像质量更高质、噪声更少，但代价是运动中的物体会产生一些运动模糊。 深度帧DepthFrame 类表示一种帧，其中每个像素代表该像素看到的最近物体的距离。此帧的数据存储为 16 位无符号整数，每个值代表以毫米为单位的距离。最大深度距离为 8 米，但可靠性在 4.5 米左右就开始下降。在 BodyFrame 类不足以满足需求时，开发者可以使用深度帧来构建自定义跟踪算法。 人体帧BodyFrame 类包含传感器视图中所有人计算出的所有实时跟踪信息，包括骨骼关节点和方向、手部状态等，最多可同时跟踪 6 人。这些跟踪功能为在应用程序中实现人机交互提供了良好的基础。要从 BodyFrame 中提取此跟踪数据，请分配一个包含 6 个 body 指针的向量，并将其传递给 BodyFrame.GetAndRefreshBodyData 方法。数组中的每个 body 代表可以同时跟踪的 6 个可能的 body 中每一个的跟踪信息。每个代表传感器视野中真实用户的 body 将被标记为“已跟踪”。 要访问 body 列表，首先在启动时分配一个向量，如下所示： 1auto bodies = ref new Platform::Collections::Vector&lt;Body^&gt;(6); 然后，对于每一帧，像这样更新向量： 1bodyFrame-&gt;GetAndRefreshBodyData(bodies); 人体索引帧BodyIndexFrame 类表示基于深度图像计算出的帧。此图像表示哪些深度或红外像素属于被跟踪的人，哪些属于背景。此帧中的像素值是 8 位无符号整数，其中 0-5 直接映射到 BodyFrame 类中的 BodyData 索引。其余从 BodyFrameSource.BodyCount 属性获得的值表示该像素是背景的一部分，与跟踪的 body 无关。此帧对于绿幕应用程序或任何想要显示用户轮廓的场景非常有用。它也为自定义深度算法提供了良好的起始边界。 色彩帧源由于需要在流上设置一些选项，彩色帧源扩展了深度流中定义的模式。使用此源可以获取转换为所需彩色图像格式（如 RGB 或 YUV）的图像帧数据副本。无论格式如何，像素类型都将是 UInt8（字节）。有不同的格式的 CopyRaw 和 LockRaw 访问器，以及新的 CopyConverted 访问器，它们将原始图像转换为提供缓冲区中的指定格式。 API 元素 描述 ColorFrameSource.CreateFrameDescription 方法 创建描述具有所提供格式和分辨率的彩色帧的 FrameDescription 对象。这取代了在其他图像流上找到的 FrameDescription 属性。 ColorFrameSource.OpenReader 方法 打开一个新的流读取器。此读取器必须被释放。 ColorFrame.CopyConvertedFrameDataToArray 方法 将原始格式转换为请求的格式并将数据复制到提供的数组中。 ColorFrame.CopyConvertedFrameDataToBuffer 方法 用转换为指定格式的图像版本填充提供的缓冲区。 ColorFrame.CopyRawFrameDataToArray 方法 将原始帧数据复制到提供的数组中。 ColorFrame.CopyRawFrameDataToBuffer 方法 将原始帧数据复制到提供的缓冲区中。 ColorFrame.RawColorImageFormat 属性 返回原始像素的颜色格式。 多源帧MultiSourceFrame 类通过单个接口提供对多种帧类型的访问。通过以下属性获取对各个帧类型的引用： MultiSourceFrame.BodyFrameReference 属性 MultiSourceFrame.BodyIndexFrameReference 属性 MultiSourceFrame.ColorFrameReference 属性 MultiSourceFrame.DepthFrameReference 属性 MultiSourceFrame.InfraredFrameReference 属性 MultiSourceFrame.LongExposureInfraredFrameReference 属性 音频数据音频波束帧音频数据从麦克风阵列收集，并处理成波束，以强调声源方向。波束可以设置为自动跟踪声源或瞄准特定方向。波束的声音数据被划分为帧，每帧大致对应于每个视频帧。声音帧又细分为约 16 毫秒长的子帧。 用于从 Kinect 传感器获取音频数据的 WinRT API 在 WindowsPreview.Kinect 命名空间下的参考中有描述。","link":"2025/09/12/Kinect API 概述/"},{"title":"个人博客搭建","text":"前置工作初期规划需求 费用：免费 类型：技术文档为主 方案 工具：GitHub Pages 和 Hexo 域名形式：xxx.github.io 搭建预备注册 GitHub 账号，安装 Git 和 Node.js 测试 123git --versionnode -vnpm -v 正常显示版本号即安装成功 搭建过程安装 Hexo 创建用于存放博客的文件夹，我这里命名为Blog_hexo_Awoken 安装 Hexo 脚手架 1npm install -g hexo-cli 初始化博客文件夹 12hexo init Blog_hexo_Awoken // 替换为你的博客文件夹名称cd Blog_hexo_Awoken 安装依赖 1npm install 测试 1hexo s // 启动服务器，默认访问网址为 http://localhost:4000 打开浏览器，可以访问http://localhost:4000即成功 生成静态文件 将写好的文章放入source/_posts/目录中（默认存在helloworld.md） 生成静态网页 12hexo clean // 清理缓存和旧的生成网页hexo g // 生成静态网页 生成的网页文件在public/文件夹 测试 1hexo s 推送到 GitHub Pages新建仓库 创建一个名为USERNAME.github.io的仓库（仓库名必须是这个格式） 初始化仓库 添加文件并创建第一次提交 123git initgit add .git commit -m &quot;第一次提交&quot; 设置远程仓库 1git remote add origin https://github.com/USERNAME/USERNAME.github.io.git 推送到 GitHub 12git branch -M mastergit push -u origin master 如果因为如下的网络连接问题推送到 GitHub 失败，需要使用代理/VPN。 1Error: Spawn failed at ChildProcess.&lt;anonymous&gt; 部署到 GitHub 以下方式二选一即可 HTTPS 方式生成 Personal Access Token 登录 GitHub → Settings → Developer settings → Personal access tokens → Tokens (classic) → Generate new token 勾选 repo 权限 复制生成的 token 修改 Hexo 配置 修改根目录下的_config.yml1234deploy: type: git repo: https://USERNAME:TOKEN@github.com/USERNAME/USERNAME.github.io.git branch: master SSH 方式 未尝试成功该方式，成功后再记录。 部署静态网页123hexo cleanhexo ghexo d 测试 打开网址https://USERNAME.github.io检查是否搭建成功 问题汇总什么时候需要git pull 只需要在初始化仓库时pull，后续上传静态网页的操作已经被包含在hexo d中了 注：本文中的仓库专指用于存放静态网页的仓库；如需要存储源码，需要另开一个仓库。 网络连接问题导致无法上传配置代理 这里使用Clash for Windows 开启系统代理 代理类型：HTTP / HTTPS 代理端口：7890 测试 如浏览器能正常打开github.com，即配置成功 强制 Git 走代理端口 安装Proxifier 注：该软件只有一个月的试用期 配置 Proxifier 代理添加代理服务器 打开 Proxifier → 菜单 Profile → Proxy Servers → Add 填写： Address：127.0.0.1 Port：7890 Protocol：HTTP 点击 Check 测试连接是否成功 若成功，点击 OK 完成添加 添加代理规则 打开 Proxifier → Profile → Proxification Rules → Add 设置规则 Name：Git Applications：Git 的可执行路径，我的路径如下（要替换成自己的路径）D:\\Software\\Git\\cmd\\git.exe Action：刚才添加的 HTTP 代理 保存规则","link":"2025/09/12/个人博客搭建/"},{"title":"人体追踪","text":"原文 本文讨论如何使用 Kinect 传感器的人体追踪功能。 人体数据帧通过 BodyFrameReader 访问的 人体数据帧（BodyFrame），提供了访问单个人体数据的方法 GetAndRefreshBodyData。此方法接收一个 IVector&lt;Body&gt; 参数，并用当前BodyFrame的值更新其中的 Body 实例。如果传入的 IVector 为 null，则该参数将被填充为一个新创建的 Body 对象。输入的向量大小必须为 BodyFrameSource::BodyCount。 此模式实现了零分配（zero-allocation）获取 Body 对象，同时允许未来对类进行扩展而不会破坏现有代码。 以下代码示例展示了如何打开传感器、订阅 BodyFrameArrived 事件，并用当前的人体数据填充 bodies 向量。 1234567891011121314151617void MainPage::InitKinect(){ KinectSensor^ sensor = KinectSensor::GetDefault(); sensor-&gt;Open(); bodyReader = sensor-&gt;BodyFrameSource-&gt;OpenReader(); bodyReader-&gt;FrameArrived += ref new TypedEventHandler&lt;typename BodyFrameArrivedEventArgs^&gt; (this, &amp;MainPage::OnBodyFrameArrived); bodies = ref new Platform::Collections::Vector&lt;Body^&gt;(6);}void MainPage::OnBodyFrameArrived(BodyFrameReader ^sender, BodyFrameArrivedEventArgs ^eventArgs){ BodyFrame ^frame = eventArgs-&gt;FrameReference-&gt;AcquireFrame(); if (frame != nullptr){ frame-&gt;GetAndRefreshBodyData(bodies); }} 人体数据自然用户界面（NUI）追踪到的身体是镜像显示的，就好像是玩家在照镜子一样。这使得玩家与游戏世界的交互更加自然。例如，要让玩家触摸屏幕右侧的一个物体，应使用其右侧的关节点。 JointType::HandRight（右手关节） JointType::HandTipRight（右手指尖关节） JointType::ThumbRight（右手拇指关节） 关节法线每个身体关节都有一个用于描述旋转的法线（normal）。旋转表示为世界空间中的一个向量，该向量垂直于该关节在层级结构中所连接的骨骼。例如，要确定右肘关节的滚转角度，需要使用其层级中的直接双亲关节（即右肩关节）来确定骨骼所在的平面。 关节层级关节层级从身体中心向四肢延伸，并且从最顶层的关节到最底层的关节依次排列。这些关节之间的连接关系称为骨骼（bones）。例如，右臂（不包含拇指）的骨骼由以下连接组成： 右手 — 右手指尖 右腕 — 右手 右肘 — 右腕 右肩 — 右肘 按照惯例，骨骼被描述为亲子连接。 什么是 HandDataHandLeftState和HandRightState属性提供了玩家的双手状态信息。你可以使用这些信息来判断玩家是否正在与游戏世界中的某个对象进行交互。 返回的手部状态包括： Open（张开） Closed（握拳） Lasso（套索） NotTracked（未追踪） Unknown（未知） 使用套索手势相较于张开和握拳手势用途的直观，套索手势显得复杂一些。套索手势是指：手握拳，同时竖起食指和中指。这是一个类似指向的手势，但需要食指和中指两根手指；也类似于一个两指并拢的剪刀手。如果手足够大且靠近摄像头，只竖起一根手指也有可能被识别为套索。要求使用两根手指是为了在深度数据上形成足够大的特征以便系统能够识别。 套索状态没有严格的使用规则。它可以被应用在如下手势机制中： 用圆形圈出选择区域 激活某个区域，比如屏幕上的某个对象 在屏幕上绘画 旋转屏幕对象（例如，在角色创建过程中旋转玩家的虚拟角色） 其应用方式多种多样，具体取决于游戏设计和预期的玩家体验。 表情、行为与外观这部分并非对人体数据的计算，而是为了与 Xbox One 兼容而存在的。如果你希望获取表情、行为或外观信息，请参考[[面部追踪]]。 交互程度在之前的版本中，Body::Engaged属性可以用于判断某个用户与系统的交互程度。该属性现已弃用。请参考[[面部追踪]]来获取此类数据。 图像与人体数据的空间对齐有时你可能需要在不同空间（深度、彩色、摄像机空间）中确定某些点的位置。例如，你想知道玩家的头部在彩色图像中的位置。 你可以使用CoordinateMapper这个核心工具类，该类可以从KinectSensor实例中获取。以下代码示例展示了如何从彩色空间中获取代表人体头部关节的彩色像素点ColorSpacePoint。 1234567891011121314151617181920void MainPage::OnBodyFrameArrived(Platform::Object ^sender, BodyFrameArrivedEventArgs ^eventArgs){{ BodyFrame ^frame = eventArgs-&gt;FrameReference-&gt;AcquireFrame(); if (frame != nullptr) { frame-&gt;GetAndRefreshBodyData(bodies); for each (Body ^body in bodies) { Joint headJoint = body-&gt;Joints-&gt;Lookup(JointType::Head); CameraSpacePoint headLocation = headJoint.Position; TrackingState headTrackingState = headJoint.TrackingState; CoordinateMapper ^mapper = frame-&gt;BodyFrameSource-&gt;KinectSensor-&gt;CoordinateMapper; ColorSpacePoint headPoint = mapper-&gt;MapCameraPointToColorSpace(headLocation); } }} 在滤波中使用关节追踪状态被推断出的关节因为准确性较低，更有可能包含尖峰噪声。此外，推断关节的随机噪声水平通常也更高。在实际开发 Kinect 游戏时，开发者应将关节的追踪状态视为判断关节数据质量的重要依据，当关节追踪状态为“推断”时，采用更强力的平滑滤波器。这可以通过在滤波器的实现中检查关节追踪状态和基于关节追踪状态自适应地更新滤波器参数来轻松实现。此外，当关节追踪状态为“推断”时，应针对性地采用更擅长去除尖峰噪声的滤波器。 如果你想了解更多关于关节滤波技术的内容，请阅读文档 Joint Filtering Best Practices 。 参阅[[Kinect for Windows 编程指南]]","link":"2025/09/12/人体追踪/"},{"title":"倾斜追踪","text":"原文 倾斜追踪 API 使得将玩家的倾斜（即其身体偏离垂直方向的程度）融入他们的体验成为可能。例如，你可以使用此功能让玩家倾斜身体以绕过障碍物来观察某些东西。返回的数值经过预滤波处理，在稳定性和延迟之间进行了最佳平衡，其封装形式类似于游戏摇杆 (thumbstick) API。 左右倾斜对应于 X 轴上的移动；前后倾斜对应于 Y 轴上的移动。两个方向的数值范围均在 -1 到 1 之间，其中 1 大致对应 45 度的倾斜角度。 倾斜值从人体数据帧（body frame）中获取。 从 BodyFrame 获取 X 和 Y 倾斜数值以下代码展示了如何获取身体倾斜的 X 和 Y 值： 1234567891011121314IBody^ body = // Gets the body frame using NUI APIs.Windows::Foundation::Point leanAmount = body-&gt;Lean;float x = leanAmount.X;float y = leanAmount.Y;if(body-&gt;LeanTrackingState == TrackingState::Tracked){ // Do stuff with lean values.}else{ // Use values from previous frame or default values.} 参阅[[人体追踪]]","link":"2025/09/12/倾斜追踪/"},{"title":"坐标映射","text":"原文 CoordinateMapper类CoordinateMapper 类用于执行两项任务： 将深度数据从 2D 图像空间投影到 3D 摄像机空间，或者从 3D 摄像机空间映射回 2D 图像空间 在深度图像上的位置与其在彩色图像上的对应位置之间进行映射。 摄像机空间摄像机空间指的是 Kinect 使用的 3D 坐标系。该坐标系定义如下： 原点 (x=0, y=0, z=0) 位于 Kinect 红外传感器的中心 X 轴向传感器的左侧延伸 Y 轴向上延伸（注意：此方向基于传感器的倾斜角度） Z 轴向传感器正前方延伸 1 个单位 = 1 米 图 1. 摄像机空间坐标系 请注意，这是一个 “右手”坐标系，与计算机图形学中定义摄像机空间（也称为视图空间）的方式类似。 任何在 3D 空间中运行的 Kinect 追踪算法（例如骨骼追踪）都将其结果存储在摄像机空间中。在某些情况下，你可能想要将这些点中的某一个投影到深度图像上的行/列位置上，以进行进一步处理。在这种情况下，你就需要执行从摄像机空间到深度空间的映射。 深度空间深度空间是一个用于描述深度图像上的二维位置的术语，可以将其视为一个像素的行/列位置，其中 x 是列，y 是行。因此，(x=0, y=0 )对应图像的左上角，而 (x=511, y=423) 对应图像的右下角（假设图像大小为 512×424）。在某些情况下，映射出深度空间还需要一个 z 值。对此，只需在相应的行/列处对深度图像进行采样，并直接使用该值（单位为毫米）作为 z。 深度图像上的一个常见操作是生成场景的三维点云。在这种情况下，你需要从深度空间“反投影”到摄像机空间。请注意，对深度图像的每个像素单独调用坐标映射函数的开销非常大，你需要批量处理。可以使用基于数组的函数（例如 MapDepthPointsToCameraSpace），或者获取映射表（GetDepthFrameToCameraSpaceTable）然后自己进行矩阵乘法。 如果你已知一个深度像素，并希望获取对应的红外值。这个操作很简单，因为深度和红外来自 Kinect 上的同一个传感器。只需在红外图像上对相同的行/列进行采样即可。 如果你想获取与某个深度像素对应的彩色像素，则需要使用坐标映射器将其位置转换到彩色空间。 彩色空间Kinect 上的彩色传感器与深度/红外传感器在位置上有一定偏移，因此，二者看到的场景视图略有不同。如果你想找到某个深度图像像素对应的颜色，就必须把它的位置转换到彩色空间。彩色空间描述了彩色图像上的二维位置，就像深度空间之于深度图像一样。因此，彩色空间中的一个位置是彩色图像上像素的行/列位置，其中 (x=0, y=0 )对应彩色图像的左上角，(x=1919, y=1079) 对应彩色图像的右下角（假设彩色图像大小为 1920×1080）。 从深度空间映射到彩色空间是一个常见操作，可以用于那些想要为用户生成二维彩色抠像以进行绿幕抠像或背景去除的应用程序。只需使用人体索引图像来识别哪些深度像素属于用户，然后使用坐标映射器来获取这些像素对应的颜色值。","link":"2025/09/12/坐标映射/"},{"title":"面部追踪","text":"原文 FaceFrame对象FaceFrame类提供了一组关于被追踪者面部的基本信息，包括面部的位置、视线方向、基本表情信息以及是否佩戴眼镜。所有这些信息都可以在每个被追踪的人体上进行计算，最大有效距离为 3.5 米。 FaceFrameResult对象FaceFrameResult类提供被追踪者面部基本信息，其数据在红外空间中计算，并映射到彩色空间。 FaceFrame数据为方便起见，所有FaceFrame类数据均在红外空间中计算，然后映射到彩色空间。这意味着像 FaceFrameResult.FaceBoundingBoxInColorSpace 属性的结果会受限于红外摄像头的视野。这也意味着在所有几乎所有红外流可见的照明条件下（如室内、夜晚等），都能返回所有信息。唯一的例外是 RightEyeClosed 和 LeftEyeClosed (FaceFrameResult.FaceProperties 属性)，它们依赖于彩色视频流，并且会受到不良光照条件的负面影响。 边界框数据面部检测结果包含一个边界框，这是一个由我们的面部检测算法确定的、包含用户头部的矩形。该边界框可通过 FaceFrameResult.FaceBoundingBoxInColorSpace 属性或 FaceFrameResult.FaceBoundingBoxInInfraredSpace 属性检索。 点数据点数据可通过 FaceFrameResult.FacePointsInColorSpace 属性或 FaceFrameResult.FacePointsInInfraredSpace 属性检索。这些位置也称为对齐点，代表了用户脸上的五个标志性位置。这五个点分别是：左眼、右眼、鼻子以及左右嘴角。 面部旋转四元数通过 FaceRotationQuaternion 属性获得的头部枢轴点是计算出的头部中心，面部可围绕该点旋转。该点定义在 Kinect 人体坐标系中：原点位于相机的光学中心（传感器），Z 轴指向用户，Y 轴指向上方。测量单位为米。头部枢轴点与身体的头部关节点类似，但具有不同的垂直坐标（更适合作为旋转中心）。 面部属性FaceFrameResult.FaceProperties 属性返回一个只读的键-值对映射，提供有关用户面部外观或状态的信息。对于此映射中的每个条目，键是 FaceProperty 枚举的一个成员。下表列出了可用的属性。请注意，这些分类器的训练假设 Kinect 放置在内容源（电视等）的上方或下方，居中并朝向用户。 属性（Property） 描述（Description） Happy（高兴） 用户似乎在微笑，呈现出高兴的表情。这也会捕捉到大笑时的笑容。请注意，有些用户即使不高兴也看起来像在笑，因此这不应被视为情绪的精确转换。 Engaged（投入） 结合了 LookingAway 和 EyeClosed 的结果，以确定用户是否专注于内容。 WearingGlasses（戴眼镜） 用户戴着眼镜。 LeftEyeClosed（左眼闭合） 用户的左眼是闭着的。 RightEyeClosed（右眼闭合） 用户的右眼是闭着的。 MouthOpen（嘴巴张开） 用户的嘴巴是张开的。 MouthMoved（嘴巴移动） 用户的嘴巴移动了。这是唯一一个需要跨帧结果才能做出准确判断的属性。如果用户的头部大部分时间保持静止，此功能效果最佳。 LookingAway（视线移开） 确定用户是否将视线从内容上移开。该检测不够精细，无法检测到轻微的视线转移，但可以检测到较大的头部运动，例如转头与人交谈或低头查看手机。 对于 FaceFrameResult.FaceProperties 属性映射中的每个条目，其值是 DetectionResult 枚举的一个成员，表示系统对相应面部属性的检测结果。下表描述了如何调整和计算这些结果。 检测结果 (DetectionResult) 描述 (Description) Yes (是) 我们非常确定该属性为真，你可以基于此结果采取决定性操作。 No (否) 我们非常确定该属性为假，你可以基于此结果采取决定性操作。 Maybe (可能) 我们相当确定该属性为真。你可以基于此结果奖励用户或给予正向反馈。对于大多数属性，你可以由此推断相应的动作幅度较小。 Unknown (未知) 我们没有足够的信息来做出判断。这通常是因为用户将脸转向远离传感器的方向，而我们也不想给出一个糟糕的结果。 数据库文件每个使用 Microsoft.Kinect.Face.dll 的应用程序必须与随同 Microsoft.Kinect.Face.dll 一起发布的 NuiDatabase 文件夹打包在一起。Microsoft.Kinect.Face.dll仅保证与它一同发布的特定 NuiDatabase 文件夹一起正常工作。面部 API 在初始化时会从 NuiDatabase 文件夹中加载数据库文件，并且会在 Microsoft.Kinect.Face.dll 相同的路径下查找该文件夹。 性能考虑如果要在事件处理程序中执行长时间运行的任务（耗时超过单帧到达的时间），则必须在执行任务之前获取所有相关的帧引用。如果未事先获取引用，帧数据可能会被覆盖，获取引用或数据的调用将返回 null。 通常，最好避免在事件处理程序中执行长时间运行的任务，应使用独立线程来处理。","link":"2025/09/12/面部追踪/"},{"title":"高清面部追踪","text":"原文 高清面部追踪 API 主要支持两种场景：帮助玩家融入游戏，并在玩家与游戏之间建立更强烈的情感连接。 面部捕捉 — 使用 API 获取玩家面部的形状。API 会告知游戏开发者用户需要位于摄像头视图中的哪个位置、所捕获帧的质量，并在捕获到足够多的有效帧后计算用户的面部形状并将这些数据提供给游戏开发者。随后，游戏开发者可以利用这些形状参数影响游戏角色的外观设计（例如，使玩家的游戏内角色看起来更像玩家本人）。 面部追踪 — 使用 API 获取用户的实时面部表情，用于驱动游戏内玩家角色的动画。对于每一帧，开发者都将获得与玩家面部运动相关的动画单元。这既可以应用于已塑形为类似玩家外貌的游戏角色，也可以应用于与玩家外貌不同的通用游戏虚拟形象。此功能可与面部捕捉结合使用，也可单独使用。 输入WinRT API 会自动收集其输入。当开发者启动面部捕捉或订阅面部追踪流时，WinRT API 会将 Kinect 的彩色、深度和红外图像作为输入提供给 FaceModelBuilder 或高清面部追踪。 追踪质量可能会受到这些输入帧图像质量的影响（即，较暗、模糊的帧追踪效果比明亮、清晰的帧差）。此外，较大、较近的面部比较小的面部追踪效果更好。API 仅会追踪那些 NUI 骨骼系统已识别出头部和颈部关节的帧。 如果面部已被捕捉过，并且使用捕捉的输出来初始化面部追踪，追踪质量会更高。因为此时面部追踪器使用的是用户面部的精确几何形状，而不是平均几何形状，从而获得更准确的面部运动特征。 输出面部捕捉 API 输出一个面部形状，其定义为一组应用于面部模型的参数。这些参数包括一个指示面部大小的比例因子，以及一组指示面部形状与平均形状差异程度的形状单元（SU）权重。这组形状单元用于估算用户头部的特定形状：例如眉毛、鼻子、脸颊、嘴巴或下巴等特征的形状。 在面部追踪过程中，API 会输出一个面部朝向、一个头部枢轴点以及一个动画单元列表（AUs）。 动画单元是相对于中性形状的偏移量，你可以使用它们来变形指定的动画虚拟形象模型，使虚拟形象表现出被追踪的用户的面部动作。例如，动画单元定义了嘴巴是否张开、眉毛是否抬起等面部表情细节。 朝向与位置高清面部追踪返回的面部朝向是一个四元数，其解释与 FaceRotationQuaternion 属性相同。 头部枢轴点是计算出的头部中心，面部可围绕该点旋转。该点定义在 Kinect 人体坐标系中：原点位于相机的光学中心（传感器），Z 轴指向用户，Y 轴指向上方。测量单位为米。头部枢轴点与身体的头部关节点类似，但具有不同的垂直坐标（更适合作为旋转中心）。 动画单元该 API 追踪 17 个动画单元。大多数动画单元的权重范围为 0 到 1 ，其中有三个（下颌右滑、右眉下压、左眉下压）范围为 -1 到 +1 。 动画单元的索引定义在 FaceShapeAnimations 枚举中，其数量和含义可能会有变动。 形状单元共有 70 多个形状单元。形状单元的权重范围通常在 -2 到 +2 之间，但也可能超出该范围。 有关形状单元的完整列表，请参阅 FaceShapeDeformations 枚举。形状单元的数量和含义可能会有变动。 面部模型您可以选择性地读取通过面部捕捉计算出的 3D 面部模型，包括网格中的每个顶点和三角形。 部分网格顶点具有被预定好的含义，例如鼻尖和右眼外角。有关高精度面部点的完整列表，请参阅 HighDetailFacePoints 枚举。 这些顶点的用途与 FaceFrame 类中的属性类似。不建议在屏幕上渲染此追踪模型，调试除外。","link":"2025/09/12/高清面部追踪/"}],"tags":[{"name":"Kinect","slug":"Kinect","link":"tags/Kinect/"},{"name":"翻译","slug":"翻译","link":"tags/翻译/"},{"name":"摄影","slug":"摄影","link":"tags/摄影/"},{"name":"教程","slug":"教程","link":"tags/教程/"}],"categories":[{"name":"Kinect for Windows 编程指南","slug":"Kinect-for-Windows-编程指南","link":"categories/Kinect-for-Windows-编程指南/"},{"name":"摄影","slug":"摄影","link":"categories/摄影/"},{"name":"个人博客建设","slug":"个人博客建设","link":"categories/个人博客建设/"}]}